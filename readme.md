# Installation #

## Dependencies ##

- anaconda 5.1.0
- scrapy 1.5.0
- mongodb v3.6.4
- pymongo 3.6.1
- dateparser-0.7.0

## Usage ##

### Stockopedia Account Info ###

- Update stockopedia account info `login()` method in every
  spider

### Start MongoDB ###

1. execute `mongod &` first
2. (optional) run `mongo` to start a mongo shell

### Run Spider ###

1. `cd [spider directory]`
2. `scrapy crawl [spider_name]`
   - `stock_url` for scraping all markets' all stocks' urls from
     dropdown list. Store data in db: `stockopedia` collection:
     `global_urls`
   - `news` for scraping global stocks' all historical news.
     Store data in db: `stockopedia_news` collection:
     `[ExchangeName_News]` (for example, all ASX news will be
     stored in collection `ASX_News`)
   
   - Example:
     - `cd /home/ubuntu/mqdCodeLab/news_scrapper_cmcrc/news`
     - `scrapy crawl news`

## Inspecting Scraped Results ##

Execute `mongo` to start a mongo shell instance.

### Inspecting Global Market URLs ###

``` javascript
use stockopedia
// count how many records are stored in this collection
db.global_urls.count()
// show some results
db.global_urls.find()
```

### Inspecting Market News Titles ###

``` javascript
use stockopedia_news
// count how many ASX news are scraped
db.ASX_News.count()
// show some results
db.ASX_News.find()
```



## Reuters Spider ##

### Run ###

#### Scraping ####

``` bash
cd ./reuters
scrapy crawl reuters_spider
```

#### Generate CSV ####

``` bash
cd ./reuters/utils
python gen_csv.py
```

The output file is named `result_asx_sgx_johannesburg_istanbul_sao_paulo_lse_nasdaq_2018-05-04_2018-05-31.csv`

### Input CSV ###

Spider read `./reuters/final_mqd_nodata.csv` as source file of
urls to scrape.

At 2018-06-22 when the code is wrote, mqd refdata database do not
have company name data for sao_paulo, sgx and jse. We have to use
both refdata sql database source and trth source.

File `final_mqd_nodata.csv` contains all mkt, ric, company name,
and urls to scrape. which is generated by
`./reuters/utils/db_utils.py` and
`./reuters/utils/test_simon.py`.

If refdata has complete dataset, `db_utils.py` would be enough.

## On mqd ssh server ##

### Running ###

```
tnew so_urls
cd /home/lchang/mqdCodeLab/news_scrapper_cmcrc/stockpedia/
scrapy crawl stock_url

tnew so_news
cd /home/lchang/mqdCodeLab/news_scrapper_cmcrc/news/
scrapy crawl news
- old_latest_date = dp.parse('2018 05 30') change this date to
  last running date (can be found in log file)

tnew rt_news
cd /home/lchang/mqdCodeLab/news_scrapper_cmcrc/reuters/
scrapy crawl reuters_spider


cd /home/lchang/mqdCodeLab/news_scrapper_cmcrc/utils/
cd /home/lchang/mqdCodeLab/news_scrapper_cmcrc/ex_spiders

import pickle
with open('tradable_sequence_dict.pickle', 'rb') as f:
  tradable_sequence_dict = pickle.load(f)

{'nasdaq:ZG:USD': ['NSM', 'ZG.OQ'], 'nasdaq:MASI:USD': ['NSM', 'MASI.OQ']}
https://www.mqdashboard.com/insight/merged/#search/none/none/1/none/none/none/none/none/none/none/book_value_of_equity,total_trade_value,total_trade_value_localCurrency/0/1/default/5/1/2018-04-03/2018-10-02/none/none/3/I/1/none/none/none/none/line

/home/lchang/mqdCodeLab/prototypes/BUS-3472/
```

### Inspecting ###

```
tmux ls # list session name
tat [session name]
C-Shift-\- d dettach
```

## Upload ##

### Generate CSV Files ###

```bash
cd /home/lchang/mqdCodeLab/news_scrapper_cmcrc/utils/
gen_reuters_csv.py
gen_so_csv.py

# gen_reuters_csv.py
  be_date = '2018-10-01'
  en_date = '2018-10-31'
python gen_reuters_csv.py 
# output: result_sao_paulo_johannesburg_2018-10-01_2018-10-31.csv

# gen_so_csv.py
  be_date = '2018-10-01'
  en_date = '2018-10-31'
python gen_so_csv.py
# output: result_sao_paulo_johannesburg_2018-10-01_2018-10-31.csv
```

### Test on AWS ###

```bash
scp result_asx_lse_nasdaq_sgx_sao_paulo_2018-10-01_2018-10-31.csv mqdAWS:~/Downloads

cd /home/ubuntu/mqdCodeLab/prototypes/ETL-3836/
source up/bin/activate
python convert_and_upload.py --input_file=news/result_asx_lse_nasdaq_sgx_sao_paulo_2018-09-01_2018-09-30.csv --date_from=2018-09-01 --date_to=2018-09-30
```

### Push CSV ###

``` bash
cp result_asx_lse_nasdaq_sgx_sao_paulo_2018-10-01_2018-10-31.csv /home/lchang/mqdCodeLab/prototypes/ETL-3836/news/ 
```

### Check S3 ###

- https://console.aws.amazon.com/s3/buckets/cmcrc-workflow/convert_info/asx_info/2018/?region=us-east-2&tab=overview
